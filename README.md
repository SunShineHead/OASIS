ğŸ“˜ README.md â€” Pyton Package In Conda

A fully selfâ€‘healing, auto-fixing, autoâ€‘versioned, autoâ€‘releasing MLOps system.

Â 

ğŸš€ Overview

OASIS is a fully autonomous Machine Learning + DevOps hybrid pipeline featuring:

Realâ€‘dataset LightGBM training

Versioned model saving

Semantic versioning

Full CLI toolkit (Â oasis trainÂ , Â oasis versionÂ , Â oasis autoâ€‘fixÂ , etc.)

Automatic changelog generation

Automatic GitHub Releases

CI Retry + Autoâ€‘Merge system

PRâ€‘based selfâ€‘healing

Autoâ€‘close failing PRs

Nightly autoâ€‘fix pipelines

Autoâ€‘formatting, linting, diagnostics, and repository cleanup

OASIS maintains itself â€” heals its own repo, fixes CI failures, formats code, retries CI, publishes releases, updates changelogs, and more.

Â 

ğŸ“ Project Structure

Â 
OASIS/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset.csv
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained_model.pkl
â”‚   â”œâ”€â”€ version.txt
â”‚   â””â”€â”€ history.log
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train_pipeline.py
â”‚   â”œâ”€â”€ model_loader.py
â”‚   â””â”€â”€ oasis/
â”‚       â””â”€â”€ cli.py
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_lgb_model.py
â”‚
â””â”€â”€ .github/workflows/
    â”œâ”€â”€ ci.yml
    â”œâ”€â”€ oasis-auto-fix.yml
    â”œâ”€â”€ oasis-auto-fix-pr.yml
    â”œâ”€â”€ oasis-auto-fix-nightly.yml
    â”œâ”€â”€ oasis-auto-merge.yml
    â”œâ”€â”€ oasis-auto-close.yml
    â””â”€â”€ oasis-ci-retry.yml
Â 

Â 

ğŸ§  Training Pipeline

Training uses:

Â 
src/train_pipeline.py
Â 

Pipeline includes:

Loading real dataset

Splitting training/test

Training LightGBM

Saving model + metadata

Recording semantic version

Appending version history

Train manually:

Â 
oasis train
Â 

Â 

ğŸ§ª Testing

Tests validate:

Model load

Feature alignment

Prediction behavior

Deterministic output

Run manually:

Â 
pytest -v


pytest documentation

Logo
Search
Get Started
How-to guides
Reference guides
Explanation
Examples and customization tricks
About the project

Changelog
Contributing
Backwards Compatibility Policy
History
Python version support
Sponsor
pytest for enterprise
License
Contact channels
Useful links

pytest @ PyPI
pytest @ GitHub
Issue Tracker
PDF Documentation
Get Started
Install pytest
Run the following command in your command line:

pip install -U pytest
Check that you installed the correct version:

$ pytest --version
pytest 9.0.2
Create your first test
Create a new file called test_sample.py, containing a function, and a test:

# content of test_sample.py
def func(x):
    return x + 1


def test_answer():
    assert func(3) == 5
The test

$ pytest
=========================== test session starts ============================
platform linux -- Python 3.x.y, pytest-9.x.y, pluggy-1.x.y
rootdir: /home/sweet/project
collected 1 item

test_sample.py F                                                     [100%]

================================= FAILURES =================================
_______________________________ test_answer ________________________________

    def test_answer():
>       assert func(3) == 5
E       assert 4 == 5
E        +  where 4 = func(3)

test_sample.py:6: AssertionError
========================= short test summary info ==========================
FAILED test_sample.py::test_answer - assert 4 == 5
============================ 1 failed in 0.12s =============================
The [100%] refers to the overall progress of running all test cases. After it finishes, pytest then shows a failure report because func(3) does not return 5.

Note

You can use the assert statement to verify test expectations. pytestâ€™s Advanced assertion introspection will intelligently report intermediate values of the assert expression so you can avoid the many names of JUnit legacy methods.

Run multiple tests
pytest will run all files of the form test_*.py or *_test.py in the current directory and its subdirectories. More generally, it follows standard test discovery rules.

Assert that a certain exception is raised
Use the raises helper to assert that some code raises an exception:

# content of test_sysexit.py
import pytest


def f():
    raise SystemExit(1)


def test_mytest():
    with pytest.raises(SystemExit):
        f()
Execute the test function with â€œquietâ€ reporting mode:

$ pytest -q test_sysexit.py
.                                                                    [100%]
1 passed in 0.12s
Note

The -q/--quiet flag keeps the output brief in this and following examples.

See Assertions about approximate equality for specifying more details about the expected exception.

Group multiple tests in a class
Once you develop multiple tests, you may want to group them into a class. pytest makes it easy to create a class containing more than one test:

# content of test_class.py
class TestClass:
    def test_one(self):
        x = "this"
        assert "h" in x

    def test_two(self):
        x = "hello"
        assert hasattr(x, "check")
pytest discovers all tests following its Conventions for Python test discovery, so it finds both test_ prefixed functions. There is no need to subclass anything, but make sure to prefix your class with Test otherwise the class will be skipped. We can simply run the module by passing its filename:

$ pytest -q test_class.py
.F                                                                   [100%]
================================= FAILURES =================================
____________________________ TestClass.test_two ____________________________

self = <test_class.TestClass object at 0xdeadbeef0001>

    def test_two(self):
        x = "hello"
>       assert hasattr(x, "check")
E       AssertionError: assert False
E        +  where False = hasattr('hello', 'check')

test_class.py:8: AssertionError
========================= short test summary info ==========================
FAILED test_class.py::TestClass::test_two - AssertionError: assert False
1 failed, 1 passed in 0.12s
The first test passed and the second failed. You can easily see the intermediate values in the assertion to help you understand the reason for the failure.

Grouping tests in classes can be beneficial for the following reasons:

Test organization

Sharing fixtures for tests only in that particular class

Applying marks at the class level and having them implicitly apply to all tests

Something to be aware of when grouping tests inside classes is that each test has a unique instance of the class. Having each test share the same class instance would be very detrimental to test isolation and would promote poor test practices. This is outlined below:

# content of test_class_demo.py
class TestClassDemoInstance:
    value = 0

    def test_one(self):
        self.value = 1
        assert self.value == 1

    def test_two(self):
        assert self.value == 1
$ pytest -k TestClassDemoInstance -q
.F                                                                   [100%]
================================= FAILURES =================================
______________________ TestClassDemoInstance.test_two ______________________

self = <test_class_demo.TestClassDemoInstance object at 0xdeadbeef0002>

    def test_two(self):
>       assert self.value == 1
E       assert 0 == 1
E        +  where 0 = <test_class_demo.TestClassDemoInstance object at 0xdeadbeef0002>.value

test_class_demo.py:9: AssertionError
========================= short test summary info ==========================
FAILED test_class_demo.py::TestClassDemoInstance::test_two - assert 0 == 1
1 failed, 1 passed in 0.12s
Note that attributes added at class level are class attributes, so they will be shared between tests.

Compare floating-point values with pytest.approx
pytest also provides a number of utilities to make writing tests easier. For example, you can use pytest.approx() to compare floating-point values that may have small rounding errors:

# content of test_approx.py
import pytest


def test_sum():
    assert (0.1 + 0.2) == pytest.approx(0.3)
This avoids the need for manual tolerance checks or using math.isclose and works with scalars, lists, and NumPy arrays.

Request a unique temporary directory for functional tests
pytest provides Builtin fixtures/function arguments to request arbitrary resources, like a unique temporary directory:

# content of test_tmp_path.py
def test_needsfiles(tmp_path):
    print(tmp_path)
    assert 0
List the name tmp_path in the test function signature and pytest will lookup and call a fixture factory to create the resource before performing the test function call. Before the test runs, pytest creates a unique-per-test-invocation temporary directory:

$ pytest -q test_tmp_path.py
F                                                                    [100%]
================================= FAILURES =================================
_____________________________ test_needsfiles ______________________________

tmp_path = PosixPath('PYTEST_TMPDIR/test_needsfiles0')

    def test_needsfiles(tmp_path):
        print(tmp_path)
>       assert 0
E       assert 0

test_tmp_path.py:3: AssertionError
--------------------------- Captured stdout call ---------------------------
PYTEST_TMPDIR/test_needsfiles0
========================= short test summary info ==========================
FAILED test_tmp_path.py::test_needsfiles - assert 0
1 failed in 0.12s
More info on temporary directory handling is available at Temporary directories and files.

Find out what kind of builtin pytest fixtures exist with the command:

pytest --fixtures   # shows builtin and custom fixtures
Note that this command omits fixtures with leading _ unless the -v option is added.

Continue reading
Check out additional pytest resources to help you customize tests for your unique workflow:

â€œHow to invoke pytestâ€ for command line invocation examples

â€œHow to use pytest with an existing test suiteâ€ for working with preexisting tests

â€œHow to mark test functions with attributesâ€ for information on the pytest.mark mechanism

â€œFixtures referenceâ€ for providing a functional baseline to your tests

â€œWriting pluginsâ€ for managing and writing plugins

â€œGood Integration Practicesâ€ for virtualenv and test layouts
Â 

Â 

âš™ï¸ GitHub Actions Overview

OASIS includes 7 fully autonomous workflows:

âœ” Â ci.ymlÂ 

Standard train + test workflow.

âœ” Â oasis-auto-fix.ymlÂ 

Self-heals repository on command.

âœ” Â oasis-auto-fix-pr.ymlÂ 

Creates auto-fix PRs instead of pushing changes.

âœ” Â oasis-auto-fix-nightly.ymlÂ 

Runs nightly repository healing at 2AM UTC.

âœ” Â oasis-auto-merge.ymlÂ 

Auto-merges approved auto-fix PRs only when CI is green.

âœ” Â oasis-auto-close.ymlÂ 

Auto-closes persistent failing PRs after 3 CI failures.

âœ” Â oasis-ci-retry.ymlÂ 

Retries CI up to 3 times before merging or closing.

Combined, these workflows create a self-maintaining MLOps ecosystem.

Â 

ğŸ§µ OASIS CLI Commands

Your CLI includes:

ğŸ”§ Training & Model Management

Â 
oasis train
oasis evaluate <dataset.csv>
oasis predict <input.csv>
Â 

ğŸ” Model Metadata

Â 
oasis version
oasis version --json
Â 

Metadata includes:

Semantic version

Timestamp

Feature list

Model size

File path

ğŸ§¾ Version History & Releases

Â 
oasis bump-version --level patch|minor|major
oasis history
oasis changelog
oasis release
Â 

Release automatically:

Tags Git

Generates changelog

Uploads model to GitHub Releases

ğŸ›  Autoâ€‘Fix & Formatting

Â 
oasis auto-fix
oasis auto-fix-strict
oasis format
oasis clean
Â 

ğŸ©º Diagnostics

Â 
oasis doctor
oasis doctor --json
oasis doctor --fix
oasis doctor --fix --commit --push
Â 

Doctor checks:

Python syntax

YAML health

GPU availability

Missing dependencies

Model file integrity

Git status

Auto-healing

Â 

ğŸ¤– Selfâ€‘Healing DevOps Explained

OASIS includes autonomous maintenance loops:

1ï¸âƒ£ Failure â†’ Auto-Fix PR

A CI failure triggers a repair branch & PR.

2ï¸âƒ£ Autoâ€‘Retry CI

OASIS retries CI up to 3 times.

3ï¸âƒ£ Autoâ€‘Comment Failure Reasons

Explains why CI failed directly on PR.

4ï¸âƒ£ Autoâ€‘Merge

If CI passes + PR is approved â†’ merge.

5ï¸âƒ£ Autoâ€‘Close

If CI fails 3 times â†’ PR closed with explanation.

6ï¸âƒ£ Nightly Repair

Nightly self-healing runs regardless of CI.

Â 

ğŸš€ Release Automation

Release with:

Â 
oasis release
Â 

This:

Reads semantic version

Creates Git tag

Generates changelog

Uploads model

Publishes GitHub Release

Optional:

Â 
oasis release --no-confirm
oasis release --notes "Custom message"
Â 

Â 

ğŸ§¹ Cleanup & Formatting

Run:

Â 
oasis clean
oasis format
Â 

Removes:

Caches

Build files

Logs

Model artifacts (optional)

And formats code using:

Black

isort

docformatter

Â 

ğŸ“¦ Installation

Editable mode installation:

Â 
pip install -e .
Â 

Â 

ğŸ›Ÿ Support

If you need enhancements, improvements, or more automation, extend the CLI or GitHub workflows.

Â 

ğŸ‰ Final Note

This README documents your complete autonomous ML + DevOps pipeline.
Your OASIS system is now capable of:

Training

Testing

Healing

Formatting

Releasing

Versioning

Closing

Commenting

Auto-merging

Nightly cleaning

all without human intervention.